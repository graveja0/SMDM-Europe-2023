---
title: "Incorporating New Evidence and Uncertainty"
subtitle: "Backwards Conversion, Solving for PSA Parameters and Copula-Based PSA Sampling"
editor: source
format:
  revealjs:
    transition: fade
    background-transition: fade
    incremental: true 
    footer: |
      [Back to Website](../index.html)
editor_options: 
  chunk_output_type: console
self-contained: true
bibliography: references.bib
---


```{r setup}
source("./manifest.r")
```

## Learning Objectives

1. Back-convert an existing transition probability matrix to incorporate new health states, strategies, and evidence.
2. Solve for probabilisitic sensitivity analysis (PSA) distribution parameters.
3. Sample correlated PSA distributions using copulas. 

## 1. Backwards Conversion {background-image="images/paste-7BE7C6AF.png" data-background-size="contain" background-opacity="0.2"}

- Lectures 2 and 3 emphasized the importance of the transition rate matrix as the central "hub" of a Markov model.


## Goal

![](images/paste-DE23E852.png)


## What *Rarely* Happens

![](images/paste-35C55E40.png)

## Adapting a Model Requires a Transition Rate Matrix

![](images/paste-500FB9B7.png)


## 1. Backwards Conversion {background-image="images/backwards-conversion.png" data-background-size="contain" background-opacity="1.0"}

## 1. Backwards Conversion {background-image="images/backwards-conversion.png" data-background-size="contain" background-opacity="0.13"}

- If starting a model from scratch, can simply define or estimate rates, and use them to construct the rate matrix.

## 1. Backwards Conversion

- What if we have a model that is already defined in terms of a transition probability matrix? 
- How can we convert back to a rate matrix to add new health states, transition states, accumulators, etc. as needed?
- Also useful if we want to keep everything the same, but change the model time step (e.g., see @chhatwalChangingCycleLengths2016)

## 1. Backwards Conversion

- We will next show you several ways to work backwards. 
- Boils down to solving for the continuous "generator matrix" for the observed transition probability matrix.

## A Word of Warning

- As you'll see, it's not always going to work perfectly. 
- If the original transition probability matrix was defined incorrectly (e.g., no jumpover probabilities), the generator matrix may not exist.
- Identifiability is a more general issue, however. 

## Working Example: HIV Model

Transition probability matrix: 

```{r}
mP = matrix(c(0.721, 0.202, 0.067, 0.010,
                0.000, 0.581, 0.407, 0.012,
                0.000, 0.000, 0.750, 0.250,
                0.000, 0.000, 0.000, 1.000), 
              nrow=4, byrow=TRUE,
              dimnames=list(c("Healthy", "LowCD4", "AIDS", "Death"),
                            c("Healthy", "LowCD4", "AIDS", "Death")))
mP  
```

::: nonincremental
-   From '*Decision Modelling for Health Economic Evaluation*' by Briggs, Claxton, Sculpher.
:::

## Three Methods to Solve for the Generator

::: nobullet
- A. Compute the matrix logarithm of the transition probability matrix $P$.

- B. Maximum likelihood-based approach.

- C. Convert the supplied transition probabilities to rates one-by-one.

- (You can always try 2+ of these methods to see how they compare.)
:::

## A. Matrix Logarithm of P

- In mathematical terms, the generator matrix is the matrix logarithm of the transition probability matrix. 
- A matrix has a logarithm *if and only if* if it is invertible.

## A. Matrix Logarithm of P

::: nonincremental
- In mathematical terms, the generator matrix is the matrix logarithm of the transition probability matrix. 
- A matrix has a logarithm *if and only if* if it is invertible.
:::

$$ R = \log P $$

## A. Matrix Logarithm of P

- The $\log$ can be found using spectral or eigenvalue decomposition. 
- If $V$ is a matrix where each column is an eigenvector of $P$, then

## A. Matrix Logarithm of P

::: nonincremental
- The $\log$ can be found using spectral or eigenvalue decomposition. 
- If $V$ is a matrix where each column is an eigenvector of $P$, then
:::

$$A' = V^{-1} A V$$ 

## A. Matrix Logarithm of P

::: nonincremental
- The $\log$ can be found using spectral or eigenvalue decomposition. 
- If $V$ is a matrix where each column is an eigenvector of $P$, then
:::

$$A' = V^{-1} A V$$ 

$$\log P = V (\log A') V^{-1}$$

## A. Matrix Logarithm of P

```{r}
#| echo: true
#| eval: false
V  <- eigen(mP)$vectors
iV <- solve(V)
Ap <- iV %*% mP %*% V
lAp <- diag(log(diag(Ap)), nrow(Ap), ncol(Ap))
R  <- V %*% lAp %*% iV
R[abs(R) < 1e-6 ] <- 0
dimnames(R) = dimnames(mP)
round(R,3)
```

## A. Matrix Logarithm of P

```{r}
#| echo: true
V  <- eigen(mP)$vectors
iV <- solve(V)
Ap <- iV %*% mP %*% V
lAp <- diag(log(diag(Ap)), nrow(Ap), ncol(Ap))
R  <- V %*% lAp %*% iV
R[abs(R) < 1e-6 ] <- 0
dimnames(R) = dimnames(mP)
round(R,3)
```


## A. Matrix Logarithm of P

```{r}
Rexp <-
  R %>%
  round(., 3) %>%
  data.frame() %>% 
  rownames_to_column() %>% 
  mutate_all(as.character) %>% 
  mutate(Death = cell_spec(Death,"html",color = ifelse(rowname=="LowCD4","red","black"))) 

Rexp %>% 
  kable(col.names = c("","Healthy","LowCD4","AIDS","Death"),escape = FALSE) %>% 
  kable_styling("hover", "striped", full_width = F)  %>% 
  kable_paper()


```

- Note there is a negative rate from LowCD4 {{< fa arrow-right >}} Death!
- This is a transition from Death {{< fa arrow-right >}} LowCD4 and should be moved to the other side of the matrix.

## A. Matrix Logarithm of P

```{r}
Rexp %>% 
  kable(col.names = c("","Healthy","LowCD4","AIDS","Death"),escape = FALSE) %>% 
  kable_styling("hover", "striped", full_width = F)  %>% 
  kable_paper()
```

-   The negative rate implies an identifiability issue.
-   Note rates from Healthy{{< fa arrow-right >}}AIDS, Healthy{{< fa arrow-right >}}Death are relatively small, implying these are from statistical noise in observation.

## A. Matrix Logarithm of P

-   `expm::logm` @highamFunctionsMatricesTheory2008 method returns same as eigenvalue method.
-   Tweaking rates to get a proper model is ad-hoc and difficult.


## A. Matrix Logarithm of P
::: nonincremental
-    @highamFunctionsMatricesTheory2008 method returns same as eigenvalue method.
    - `expm::logm()`
-   Tweaking rates to get a proper model is ad-hoc and difficult.
:::

```{r}
#| echo: true
#| eval: false

R = expm::logm(mP)
```


```{r}
R <- logm(mP)
dimnames(R) = dimnames(mP)
round(R,3)
```


## B. Multi-State Model Based Approach

-   Imposition of structural assumptions and fitting to pseudo-data derived from original Markov model can result in reasonable rate estimates.
-   We could assume that a patient with HIV at this point in time only gets sicker and external causes of death are negligible. Healthy {{< fa arrow-right >}} LowCD4 {{< fa arrow-right >}} AIDS {{< fa arrow-right >}} Death constrains model.
-   We use the reported data from the original model at year 1.

## B. Multi-State Model Based Approach

Steps:

1. Run the original model for 1+ cycles to obtain the Markov trace. 
2. Construct psuedo-data from the resulting trace based on a cohort with reasonable size (e.g., 1,000 patients)
3. Estimate transition hazards in the pseudo-data based on a multistate model.^[https://chjackson.github.io/msm/msmcourse/]
4. Use the estimated transition hazards to construct the rate matrix.

## 1. Run the original model  

```{r}
#| echo: true
#| eval: true

tr0 <- # Initial state occupancy.
  c("Healthy" = 1000,"LowCD4" = 0, "AIDS" = 0, "Death" = 0)
tr1 <- # State occupancy after one cycle.
  tr0 %*% mP 

tr <- # Bind together into a data frame. 
    rbind.data.frame(tr0, tr1) %>%
    mutate(cycle = c(0, 1)) %>% 
    select(cycle,everything())
tr

```

## 2. Construct psuedo-data

- Idea: create data for 1,000 simulated "patients" followed for two cycles.
- Counts in the Markov trace govern state occupancy in each cycle.
  - In first cycle, all 1,000 are in `Healthy` state.
  - In second cycle, 721 remain in `Healthy` while 202 are in `LowCD4`, etc.

## 2. Construct psuedo-data

::: columns

::: {.column width="66%"}
```{r}
#| echo: true
#| eval: true

tr <- 
    rbind.data.frame(tr0, tr1) %>%
    mutate(cycle = c(0, 1)) %>% 
    gather(state,count,-cycle) %>% 
    mutate(count = round(count,0)) %>% 
    arrange(cycle) %>% 
    lapply(.,rep,.$count) %>% 
    cbind.data.frame() %>% 
    as_tibble() %>% 
    mutate(state = factor(state,
      levels = c("Healthy","LowCD4","AIDS","Death"))) %>% 
    arrange(cycle,state) %>% 
    mutate(ptnum = rep(1:1000,2)) %>% 
    select(-count) %>% 
    arrange(ptnum,cycle) %>% 
    mutate(state = as.numeric(state)) %>% 
    select(ptnum,cycle,state)
```

:::

::: {.column width="33%"}
```{r}
tr 
```

:::


:::

## 3. Fit Multistate Model

- Next, fit a multistate model to the pseudo-data.
- The multistate model will use likelihood-based methods to estimate transition intensities among health states in the data.
- These transition intensities are the rates that you can use in the rate matrix!

## 3. Fit Multistate Model

::: nonincremental
1. Define the state table for the multistate model
:::

```{r}
#| echo: true
#| eval: true
library(msm)
statetable.msm(state, ptnum, data=tr)
```

::: nonincremental
2. Check that cell transition counts match the trace. 
:::
```{r}
#| echo: true
# Markov trace
rbind.data.frame(tr0, tr1) %>%
    mutate(cycle = c(0, 1)) %>% 
    select(cycle,everything())
```

## 3. Fit Multistate Model

::: nonincremental
- Initial rate guesses are based on the eigenvalue method, i.e., result of `expm(mP)`.
:::
```{r}
#| echo: true
Q.init <- rbind(rbind(c(0, 0.3, 0,   .0001),    
                      c(0, 0,   0.6, 0.01),    
                      c(0, 0,   0,   0.3),  
                      c(0, 0,   0,   0)))
dimnames(Q.init) = dimnames(mP)
```

::: nonincremental
- Fit the model
:::

```{r}
#| echo: true
hiv.msm <- 
  msm(state ~ cycle, subject = ptnum, data = tr, qmatrix = Q.init)
```

## 3. Fit Multistate Model

```{r}
#| echo: true
#| eval: true
hiv.msm
```

## 4. Construct the Rate Matrix

```{r}
#| echo: true
#| eval: true
Rmsm <- msm::qmatrix.msm(hiv.msm, ci = "none")
round(Rmsm,3)
```

## 5. Embed the Transition Probability Matrix

::: columns

::: {.column}

Multistate-Model:

```{r}
#| echo: true
#| eval: true
round(expm(Rmsm),3)
```
:::

::: {.column}

Original Model Matrix:
```{r}
#| echo: true
#| eval: true
round(mP,3)
```
:::

:::

## Markov Trace Comparison

```{r}
#| echo: true

# Original Model
c(1000, 0, 0, 0) %*% mP

# MSM-Based Model
c(1000, 0, 0, 0) %*% pmatrix.msm(hiv.msm, t=1)
```

-  Resulting expectation after 1 cycle is nearly identical.
-  The msm method has bits down at 4th decimal. Round off made them identical.
-  Only a tiny bit of error (721 vs. 721.0005) makes `log` numerically unstable converting from probability to rate.

## Key Takeaways

- By imposing some constraints on the underlying transitions, we were able to yield a generator matrix that makes sense!
- Critically, our multistate model-based generator matrix closely approximates the original transition probability matrix, but does not imply negative rates that bring people back from death. 

## Key Takeaways

::: columns

::: {.column}

Multistate-Model:

```{r}
#| echo: true
#| eval: true
round(logm(mP),3)
```
:::

::: {.column}

Original Model Matrix:
```{r}
#| echo: true
#| eval: true
round(Rmsm,3)
```
:::

:::

## Key Takeaways

- With a reasonable generator matrix defined, we can now augment the model as we see fit:

  - Add additional health states.
  - Add new strategies with evidence on efficiacy drawn from the literature (e.g., hazard rates).
  - Add accumulators and transition states.
  - Change the cycle length. 

- Once  done with the above, just embed the new matrix. 


# 2. Solving for PSA Distributions

## A Common Issue

- We have defined our underlying model (either from bottom up or through backwards conversion) but need to define PSA distributions for model parameters. 

- Model draws on literature-based parameters that are reported as means, standard deviations, interquartile ranges, 95% confidence intervals, etc.

## A Common Issue

- Straightforward to obtain base case values from literature.

- But how can we define PSA distributions based on limited information?

## PSA Distributions

| Parameter Type                  | Distribution     |
|---------------------------------|------------------|
| Probability                     | beta             |
| Rate                            | gamma            |
| Utility weight                  | beta             |
| Right skew (e.g., cost)         | gamma, lognormal |
| Relative risks or hazard ratios | lognormal        |
| Odds Ratio                      | logistic         |


## Example

- Cost parameter reported in literature has interquartile range of \$300-\$750.
- What are the parameters of a PSA distribution such that the 25th percentile is \$300 and the 75th percentile is \$750?


## Some Options

- Analytic formulas for some distributions (normal, gamma, beta).
- Formulas take as their inputs the two values ($x_1$, $x_2$) and their associated quantiles ($p_1$,$p_2$).
- Formulas return the PSA distribution parameters that match up to these values.

## Some Options
- ParameterSolver implemented in Windows, [Link](https://biostatistics.mdanderson.org/SoftwareDownload/SingleSoftware/Index/6)

- See @cookDeterminingDistributionParameters2010 and [Vasco Grilo's blog post](https://forum.effectivealtruism.org/posts/tvTqRtMLnJiiuAep5/how-to-determine-distribution-parameters-from-quantiles) for more. 

- The next few slides provide you with nearly all the tools you'll need, however. 

## Example: Cost PSA Distribution

- Suppose the interquartile range for a key cost variable is [300,750]
- We may have obtained this from the literature, from tabulating cost data, or from expert opinions.


## Example: Cost PSA Distribution

::: nonincremental
- Suppose the interquartile range for a key cost variable is [300,750]
- We may have obtained this from the literature, from tabulating cost data, or from expert opinions.
:::

```{r}
#| echo: true
x1 = 300
p1 = 0.25

x2 = 750
p2 = 0.75
```

## Analytic Solution: Uniform PSA 

For a uniform distribution with minimum $a$ and maximum  $b$

$$
a = \frac{p_2x_1 - p_1x_2}{p_2-p_1}
$$
$$
b = \frac{(1-p_1)x_2-(1-p_2)x_1}{p_2-p_1}
$$

## Analytic Solution: Uniform PSA 

```{r}
#| echo: true
a = ((p2*x1) - (p1 * x2)) / (p2 - p1)
a
b = ((1 - p1)*x2 - (1 - p2)*x1) / (p2 - p1)
b
```

## Analytic Solution: Uniform PSA 

```{r}
#| echo: true
qunif(0.25, min = 75, max = 975)
qunif(0.75, min = 75, max = 975)
```

```{r}
#| fig-align: center
plot(density(runif(n = 1e5, min = a, max = b)),main = "")
```

## Analytic Solution: Normal PSA

$$
\sigma = \frac{x_2 - x_1}{\Phi^{-1}(p_2)-\Phi^{-1}(p_1)}
$$ 

$$
\mu = \frac{x_1\Phi^{-1}(p_2)-x_2\Phi^{-1}(p_1)}{\Phi^{-1}(p_2)-\Phi^{-1}(p_1)}
$$

## Analytic Solution: Normal PSA

```{r}
#| echo: true
mu = (qnorm(p2)*x1 - qnorm(p1)*x2) / (qnorm(p2)-qnorm(p1))
mu
sigma = (x2 - x1) / (qnorm(p2) - qnorm(p1))
sigma
```

## Analytic Solution: Normal PSA

```{r}
#| echo: true
qnorm(0.25, mean = 525, sd = 333.59)
qnorm(0.75, mean = 525, sd = 333.59)
```

```{r}
#| fig-align: center
plot(density(rnorm(n = 1e5,  mean = mu, sd = sigma)),main = "")
```


## Analytic Solution: Lognormal PSA

- Just take log of $x_1$ and $x_2$

$$
\sigma = \frac{\ln(x_2) - \ln(x_1)}{\Phi^{-1}(p_2)-\Phi^{-1}(p_1)}
$$ 

$$
\mu = \frac{\ln(x_1)\Phi^{-1}(p_2)-\ln(x_2)\Phi^{-1}(p_1)}{\Phi^{-1}(p_2)-\Phi^{-1}(p_1)}
$$

## Analytic Solution: Lognormal PSA

```{r}
#| echo: true
mu = (qnorm(p2)*log(x1) - qnorm(p1)*log(x2)) / (qnorm(p2)-qnorm(p1))
mu
sigma = (log(x2) - log(x1)) / (qnorm(p2) - qnorm(p1))
sigma
```

## Analytic Solution: Lognormal PSA

```{r}
#| echo: true
qlnorm(0.25, mean = 6.1619, sd = 0.67925)
qlnorm(0.75, mean = 6.1619, sd = 0.67925)
```

```{r}
#| fig-align: center
plot(density(rlnorm(n = 1e5,  mean = mu, sd = sigma)),main = "")
```

## Analytic Solution: Gamma PSA

- A bit more involved as it involves finding the root of a function. 

## Analytic Solution: Gamma PSA
::: nonincremental
- A bit more involved as it involves finding the root of a function. 
:::

```{r}
#| echo: true
#| fig-align: center
gamma_fn <- function(alpha) {
    x1*qgamma(p2,shape = alpha, scale =1) - x2 * qgamma(p1, shape = alpha, scale = 1)
}
curve(gamma_fn, xlim = c(1,10), col = "blue", lwd = 1.5, lty=2)
abline(a=0,b=0)
```

## Analytic Solution: Gamma PSA

::: nonincremental
- Root (i.e., point where the function crosses the zero line) seems to be between 2 and 4.
:::

```{r}
#| echo: true
#| fig-align: center
gamma_fn <- function(alpha) {
    x1*qgamma(p2,shape = alpha, scale =1) - x2 * qgamma(p1, shape = alpha, scale = 1)
}
curve(gamma_fn, xlim = c(1,10), col = "blue", lwd = 1.5, lty=2)
abline(a=0,b=0)
```


## Analytic Solution: Gamma PSA

::: nonincremental
- We'll search in this range for our $\alpha$ value. 
:::

```{r}
#| echo: true
#| fig-align: center
gamma_fn <- function(alpha) {
    x1*qgamma(p2,shape = alpha, scale =1) - x2 * qgamma(p1, shape = alpha, scale = 1)
}
curve(gamma_fn, xlim = c(1,10), col = "blue", lwd = 1.5, lty=2)
abline(a=0,b=0)
```


## Analytic Solution: Gamma PSA

```{r}
#| echo: true
alpha_ <- uniroot(gamma_fn,c(2,4))$root
alpha_

calc_beta <- function(x1,p1,alpha) {
    x1 / qgamma(p1,alpha,1)
}

beta_ <- calc_beta(x1 = x1,  p1 = p1, alpha = alpha_)
beta_
```

## Analytic Solution: Gamma PSA

```{r}
#| echo: true
qgamma(0.25,shape = 2.4558, scale = 230.16)
qgamma(0.75,shape = 2.4558, scale = 230.16)
```

```{r}
#| fig-align: center
plot(density(rgamma(n = 1e4, shape = alpha_, scale = beta_)),main = "")
```


## Alternative: Optimization

::: nonincremental
-  General solution that can be used to solve for parameters of many common PSA distributions (e.g., beta, gamma, etc.).
:::
 
$$\min_{\vec{\theta} \in R}\,\, (F(x_1|\vec{\theta})-p_1)^2+(F(x_2|\vec{\theta})-p_2)^2$$

## Alternative: Optimization

::: nonincremental
-  Requires cumulative distribution function (CDF), $F$.
-  See Borchers' [great slides](https://hwborchers.lima-city.de/Presents/ROptimSlides4.pdf) for tips on optimization in R.
:::

## Some Advice

- We have found that while analytically correct, this optimization formula is not numerically stable.
- Transfinite scaling stabilizes optimization.

$$\min_{\vec{\theta} \in R}\,\, (F(x_1|\vec{\theta})-p_1)^2+(F(x_2|\vec{\theta})-p_2)^2$$

<!-- -  Expected 1.30000 0.19898 -->
<!-- -  **Not the right answer!** -->

<!-- ```{r} -->
<!-- #| echo: true -->
<!-- library(pracma) -->
<!-- norm <- function(x1, p1, x2, p2, mu, sigma) -->
<!--  (pnorm(x1, mu, sigma)-p1)^2 + -->
<!--  (pnorm(x2, mu, sigma)-p2)^2 -->

<!-- fn <- function(x) norm(0.91, 0.025, 1.69, 0.975, x[1], x[2]) -->

<!-- optim(c(0.5, 0.1), -->
<!--       fn, -->
<!--       gr = function(x) pracma::grad(fn, x), -->
<!--       lower=c(-Inf, 1e-4), -->
<!--       method = "L-BFGS-B", -->
<!--       control=list(factr=1e-10, maxit=100))$par -->

<!-- ``` -->



<!-- ## Normal via Optim -->

<!-- ::: nonincremental -->
<!-- -  Note that the function is on the probability scale. -->
<!-- -  We have seen issues with probability before. -->
<!-- -  Note the gradient of the tails.   -->

<!-- ::: -->

<!-- ```{r} -->
<!-- curve(pnorm(x), from=-10, to=10, lwd=3, col='red') -->
<!-- ``` -->

## Stable Optimization

- For probabilities, this is the $\text{logit}(p)=\log \frac{p}{1-p}=\log p - \log (1-p)$.
- Tweaking parameters to converge can still happen.
- Example of theory versus practice.

## Stable Optimization

::: nonincremental
- For probabilities, this is the $\text{logit}(p)=\log \frac{p}{1-p}=\log p - \log (1-p)$.
- Tweaking parameters to converge can still happen.
- Example of theory versus practice.
:::


$$\min_{\vec{\theta} \in R} \sum_{i \in 1,2}\,\, (\text{logit} (F(x_i|\vec{\theta}))-\text{logit} (p_i))^2$$

## Transfinite Example

```{r}
#| echo: true
Tf <- function(x, shape, rate)
    pgamma(x,shape,rate,log=TRUE) - 
    pgamma(x,shape,rate,log=TRUE, lower.tail = FALSE)

norm <- function(x1, p1, x2, p2, shape,rate)
    (Tf(x1, shape, rate)-(log(p1)-log(1-p1)) )^2 +
    (Tf(x2, shape, rate)-(log(p2)-log(1-p2)) )^2

fn <- function(x) norm(x1, p1, x2, p2, x[1], x[2])

gamma_optim <- 
  optim(c(0.5, 0.1), # initial parameter guesses
    fn,
    gr = function(x) pracma::grad(fn, x),
    lower=c(-Inf, 1e-5),
    method = "L-BFGS-B",
    control=list(factr=1e-10, maxit=100))$par
```

## Transfinite Example

::: nonincremental
- Optimization returns $\alpha$ and $1/\beta$ for the gamma distribution example.
:::

```{r}
#| echo: true

# Analytic formula
alpha_

# Optimization
gamma_optim[[1]]
```

```{r}
#| echo: true

# Analytic formula
beta_

# Optimization
1/gamma_optim[[2]]

```


## Optimization for Other Distributions

::: nonincremental
- Just swap in the distribution function for the distribution you want to match to (e.g., `pbeta` rather than `pgamma`).
:::
- Also make sure the supplied parameter names match the distribution you're aiming for. 

```{r}
#| eval: false
#| echo: true

Tf <- function(x, shape, rate)
    pgamma(x,shape,rate,log=TRUE) - 
    pgamma(x,shape,rate,log=TRUE, lower.tail = FALSE)

```

## Summary on Distribution Fitting

- Use analytical formulas if they exist.
- Optimize on $\text{logit}$ scale.
- Again, avoid $\log$ on probabilities, use `log=TRUE`.
- Plot results for visual check.


# Copula PSA Sampling

## Sensitivity Analysis for Decision Models

- It is standard in CEA analyses to explore sensitivity of model outputs and outcomes to variation in model inputs.
- Many decision problems are complicated by dependencies between model inputs (e.g., costs, utilities, etc.). 
- These correlations may have meaningful impacts on model results and sensitivity. 

## HIV Model

In the HIV model discussed earlier, patients accrue two types of costs:

1. Direct medical costs associated with each health state (`Healthy`, `LowCD4`, `AIDS`)
2. Community medical costs associated with each health state. 

## HIV Model

- These costs are likely correlated with each other in meaningful ways.
- Similarly, other model parameters (e.g., state utility payoffs, hazard rates for treatment strategies, etc.) may also be correlated.

## HIV Model

- How can we reflect this correlation in *n-way* sensitivity analyses, such as PSAs?

## What You Were Taught

- Nearly all PSA exercises draw from independent distributions.
- To fully explore the space of correlated parametersl, you need a *lot* of PSA draws.

## What You Were Taught

- PSA draws of two model parameters
  - $c_{direct} \sim \text{gamma}(2.5, 1/230)$
  - $c_{community} \sim \text{gamma}(1.5, 1/150)$

```{r}
#| fig-align: center
x1 <- rgamma(n = 1e3, shape = 2.5, rate  = 1/230.16)
x2 <- rgamma(n = 1e3, shape = 1.5, rate = 1/150)

tibble(x1 = x1, x2 = x2) %>% 
  ggplot(aes(x = x2, y = x1)) +
  geom_point(colour = "red") + 
  hrbrthemes::theme_ipsum_pub(base_family = "Arial") + 
  labs(x = "Direct Medical Costs", y = "Community Costs")
```

## What We'll Teach You

- $\text{cor}(c_{direct},c_{community}) = 0.8$
  
```{r}
# 1. Define the correlation matrix
  rho <- 0.8
  sigma <- matrix(c(1,rho,rho,1),nrow = 2, byrow=TRUE)

# 2. Perform a cholesky decomposition
  A = chol(sigma)

# 3. Generate iid standard normal pseudo random variables
  tildeY0 <- rnorm(n = 1e3, mean = 0, sd = 1)
  tildeY1 <- rnorm(n = 1e3, mean = 0, sd = 1)
  tildeY <- cbind(tildeY0,tildeY1)

  # Collect them 
  tildeY <- tildeY %*% A

# Use the standard normal disribution function to return the quantiles
  U <- pnorm(tildeY)

# Use the inverse distribution function to return the values
  Y <- U 
  cx1 <- qgamma(U[,1],shape = 2.5, rate  = 1/230.16)
  cx2 <- qgamma(U[,2],shape = 1.5, rate = 1/150)

tibble(x1 = cx1, x2 = cx2) %>% 
  ggplot(aes(x = x2, y = x1)) +
  geom_point() + 
  hrbrthemes::theme_ipsum_pub(base_family = "Arial") + 
  labs(x = "Direct Medical Costs", y = "Community Costs")
```

## What We'll Teach You

```{r}

tibble(x1 = x1, x2 = x2) %>% 
  ggplot(aes(x = x2, y = x1)) +
  geom_point(alpha = 0.5, colour = "red") + 
  geom_point(data = tibble(x1 = cx1, x2 = cx2) , aes(x = cx2, y = cx1),alpha = 0.25) +
  hrbrthemes::theme_ipsum_pub(base_family = "Arial") + 
  labs(x = "Direct Medical Costs", y = "Community Costs")  + 
  stat_ellipse(
    data = MASS::mvrnorm(n=1e3, mu = c(1500,750),Sigma = matrix(c(40000,0,0,20000),byrow=TRUE,nrow=2)) %>% 
      data.frame() %>% 
      set_names(c("x1","x2")) , 
    lty = 3, colour = "black"
  ) + 
  annotate("text", x = 750, y = 1500, label = "Traditional PSA draws (red points)\n very sparse in this region.",fontface =2, color = "red")
```


## Copula-Based PSA Sampling

- We can easily draw correlated PSA samples via copulas. 

- Copulas allow us to sample from the *joint* distribution of correlated parameters

$$
F(x_1,...,x_d) = C \big ( F_1(x_1), ..., F_d(x_d) \big )
$$

## Copulas {auto-animate="true"}

-   Copula: a multivariate cumulative distribution function with uniform marginals.
-   Intuition: the binding "glue" between random variables.


## References 

```{=html}
<style>
.nobullet li {
  list-style-type: none;
}
</style>
```

